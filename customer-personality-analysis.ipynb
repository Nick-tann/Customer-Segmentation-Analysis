{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Customer Segmentation Analysis Project\n\n# Problem Statement\n\nThis Customer Personality Analysis is a detailed analysis of a company’s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers.\n\\n\nCustomer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company’s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment.\n\n## Attributes\n\n### People\n- ID: Customer's unique identifier  \n- Year_Birth: Customer's birth year  \n- Education: Customer's education level  \n- Marital_Status: Customer's marital status  \n- Income: Customer's yearly household income  \n- Kidhome: Number of children in customer's household  \n- Teenhome: Number of teenagers in customer's household  \n- Dt_Customer: Date of customer's enrollment with the company  \n- Recency: Number of days since customer's last purchase  \n- Complain: 1 if the customer complained in the last 2 years, 0 otherwise  \n\n### Products\n\n- MntWines: Amount spent on wine in last 2 years  \n- MntFruits: Amount spent on fruits in last 2 years  \n- MntMeatProducts: Amount spent on meat in last 2 years  \n- MntFishProducts: Amount spent on fish in last 2 years  \n- MntSweetProducts: Amount spent on sweets in last 2 years  \n- MntGoldProds: Amount spent on gold in last 2 years  \n\n### Promotion\n\n- NumDealsPurchases: Number of purchases made with a discount  \n- AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise  \n- AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise  \n- AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise  \n- AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise  \n- AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise  \n- Response: 1 if customer accepted the offer in the last campaign, 0 otherwise  \n\n### Place\n\n- NumWebPurchases: Number of purchases made through the company’s website  \n- NumCatalogPurchases: Number of purchases made using a catalogue  \n- NumStorePurchases: Number of purchases made directly in stores  \n- NumWebVisitsMonth: Number of visits to company’s website in the last month  ","metadata":{}},{"cell_type":"markdown","source":"# Introduction\n\nFor this project, I will be clustering customers into segments using unsupervised learning on data from a marketing campaign. This allows us to derive insights into customer behaviour and sentiments of the products offered by this company.\n\n## Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime\nfrom datetime import date\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom yellowbrick.cluster import KElbowVisualizer\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check and prepare data","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"../input/customer-personality-analysis/marketing_campaign.csv\",sep='\\t')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Summary statistics of dataset\ndata.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking data types and possible missing values\ndata.info()\nnp.sum(pd.isnull(data))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 24 customers with missing income data. While this comes as no surprise, we will need to identify the reason for the missing data to decide how we deal with it. \n1. If they were unemployed and left it blank, the values should be imputed with 0 instead\n2. If they intentionally left it blank due to confidentiality reasons or are self-employed, then there is a good reason to leave them out of the dataset\n","metadata":{}},{"cell_type":"code","source":"#Examine observations with null income\ndata.loc[pd.isnull(data.Income)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset is constrained by the fact that employment status was not collected, so we are unable to determine which customer has 0 income. We can try to make a wild guess that if the customer is married and has children at home, they may have decided to voluntarily leave the workforce to be a caregiver, hence having 0 income. However, given that there are only 24 such observations out of 2240, representing only 1% of our dataset, it might be better to leave them out.  \n\nJust a note, for cases where the number of observations with missing values is sizeable, it would be better to consider imputations instead.","metadata":{}},{"cell_type":"code","source":"#Drop rows with missing income data\ndata=data.dropna()\n\n\n#Sense check of data values\n\ndata.nunique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For this sense check, I am looking out for attributes that defy common sense. For example, if Year_Birth has more than 120 unique values, this would be a sign that the data may have been collected incorrectly or customers may have input the wrong values.\n\n\nAt a glance, most values in the dataset make sense. *Year_birth* has 59 unique values, which is an acceptable range. *Income* has the widest range of 1974 unique values, but that is to be expected. Maximum number of kids and teens at home are 3 each. Binary values such as *AcceptedCmp* have 2 values each, which is consistent. Overall, the data looks fine.  \n\nSomething which caught my attention was *Z_CostContact* and *Z_Revenue*, which only has 1 value in each column. There is no Data Dictionary available to explain these 2 columns, but seeing as they have the same value for all observations, they would not affect our model or results if we drop them.\n\n*Marital_Status* has 8 unique values, and *Education* has 5 unique values. It might be worthwhile to explore what the different categories are for each attribute and identify any overlaps. This will be done in the next phase when we transform data.\n","metadata":{}},{"cell_type":"code","source":"data=data.drop(columns=[\"Z_CostContact\", \"Z_Revenue\"],axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Transform data\n\nAfter checking through and ensuring the sensibility and integrity of the dataset, we move on to transforming the data meaningfully. This involves identifying opportunities to group or create new attributes in order to improve our analysis.  \n\n## Categorical Attributes\n\n### Marital_Status","metadata":{}},{"cell_type":"code","source":"#Exploring Marital_Status categories\ndata['Marital_Status'].value_counts()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can group Marital Status into 2 buckets: Attached & Single. \n1. 'Married' and 'Together' can be replaced with 'Attached'\n2. 'Divorced','Widow' and 'Alone' can be replaced with 'Single' \n\nThe last 2 categories, 'YOLO' and 'Absurd', do not seem to be appropriate answers for this question. Since there are only 4 observations with such answers, I will drop them.","metadata":{}},{"cell_type":"code","source":"#Replace values\ndata['Marital_Status']=data['Marital_Status'].replace(['Married','Together'],'Attached')\ndata['Marital_Status']=data['Marital_Status'].replace(['Divorced','Widow','Alone'],'Single')\n\n#Drop rows with answers as 'YOLO' or 'Absurd' for Marital Status\ndata=data[(data.Marital_Status!='YOLO')&(data.Marital_Status!='Absurd')]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Education\n\nNext, we examine the Education attribute. We can group the 5 categories into 2 buckets: Postgrad & Undergrad\n\n1. 'Graduation','Master' and 'PhD' can be replaced with 'Postgrad'\n2. 'Basic' and '2n Cycle' can be replaced with 'Undergrad'","metadata":{}},{"cell_type":"code","source":"#Examine categories\ndata['Education'].value_counts()\n\n#Replace values\ndata['Education']=data['Education'].replace(['Graduation','Master','PhD'],'Postgrad')\ndata['Education']=data['Education'].replace(['Basic','2n Cycle'],'Undergrad')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dt_Customer\n\nUpon inspection, this is not a categorical attribute, but a datetime attribute parsed in as strings. We can convert them to datetime format here.","metadata":{}},{"cell_type":"code","source":"data['Dt_Customer'] = pd.to_datetime(data.Dt_Customer)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature Engineering\n\nWe can create useful attributes that would improve the analysis.","metadata":{}},{"cell_type":"code","source":"#Given the birth year of customers, we can derive their age during the time of data collection.\ndata['Age']=2015-data['Year_Birth']\n\n#Number of children and number of teenagers in customer's household can be combined\ndata['Children']=data['Kidhome']+data['Teenhome']\n\n#Derive number of months that customers have been members of the company as of data collection\nlast_date=date(2015,1,1)\ndata['Member_months']=pd.to_numeric(data['Dt_Customer'].dt.date.apply(lambda x:(last_date-x)).dt.days,downcast='integer')/30\n\n\n\n#Aggregate spending of all goods\ndata['Spending']=data['MntWines']+data['MntFruits']+data['MntMeatProducts']+data['MntFishProducts']+data['MntSweetProducts']+data['MntGoldProds']\n\n#Rename columns for better readability\ndata=data.rename(columns={'MntWines': 'Wines','MntFruits':'Fruits','MntMeatProducts':'Meat','MntFishProducts':'Fish','MntSweetProducts':'Sweets','MntGoldProds':'Gold'})\n\n#Create an indicator of whether customer is a parent\ndata['Is_parent']=np.where(data.Children>0,'Yes','No')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lastly, remove the unnecessary attributes\nto_delete=['Recency','ID','Year_Birth','AcceptedCmp1' , 'AcceptedCmp2', 'AcceptedCmp3' , 'AcceptedCmp4','AcceptedCmp5', 'Response', 'Kidhome', 'Teenhome','NumDealsPurchases', 'NumWebPurchases',\n       'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth','Complain','Dt_Customer']\ndata=data.drop(columns=to_delete,axis=1)\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# Visualize Data\n\nThis segment of the project will be split into 2 parts:\n1. Examine data points for each attribute to identify outliers, if any. Outliers can distort statistical analyses, so it would be important to remove them.\n2. Examine relationships between some variables.\n","metadata":{}},{"cell_type":"markdown","source":"## Univariate","metadata":{}},{"cell_type":"code","source":"#Income\n\nsns.scatterplot(x='Income',y='Income',data=data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! That's a really high annual income. While I do applaud that customer for success in his/her career, it is an extreme outlier in our dataset and will be removed.","metadata":{}},{"cell_type":"code","source":"#Remove data point\ndata=data[data['Income']<600000]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Age\n\nsns.scatterplot(x='Age',y='Age',data=data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For Age, most of the ages fall between the range of 20 to 80. The 3 outliers with ages above 100 can be removed from our dataset.","metadata":{}},{"cell_type":"code","source":"data=data[data['Age']<100]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Spending\n\nsns.scatterplot(x='Spending',y='Spending',data=data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Spending does not have any outliers, so we can leave it as it is.","metadata":{}},{"cell_type":"markdown","source":"## Bivariate","metadata":{}},{"cell_type":"code","source":"#Income and Spending\n\nsns.jointplot(x='Income',y='Spending',data=data,kind='reg')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Income and Spending seem to be positively correlated, which makes sense as customers have higher purchasing power when income is higher.","metadata":{}},{"cell_type":"code","source":"#Age and Income\n\nsns.jointplot(x='Age',y='Income',data=data,kind='reg')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Age and Income are positively correlated as well, which also makes sense as workers tend to have periodical pay increments.","metadata":{}},{"cell_type":"markdown","source":"Alternatively, we could have checked our variables using a pairplot. However, a pairplot may be too cluttered if we introduce more variables, or have regression lines per plot.","metadata":{}},{"cell_type":"code","source":"examine_vars=['Income','Spending','Age','Member_months','Is_parent']\nsns.set(rc = {'figure.figsize':(12,8)})\nsns.pairplot(data[examine_vars],hue='Is_parent')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Identify Perfectly Collinearity/ Multicollinearity amongst variables","metadata":{}},{"cell_type":"code","source":"sns.set(rc = {'figure.figsize':(10,8)})\nsns.heatmap(data.drop(['Wines', 'Fruits', 'Meat',\n       'Fish', 'Sweets', 'Gold'],axis=1).corr(),annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the correlation heatmap, there does not seem to be any case of multicollinearity or perfect collinearity.","metadata":{}},{"cell_type":"markdown","source":"# Analysis\n\n1. Label Encoding categorical attributes\n2. Scaling data so that each attribute has a single unit variance","metadata":{}},{"cell_type":"code","source":"#Identify categorical variables\ncat_vars=list(data.select_dtypes(['object']).columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Label Encoding\nLE=LabelEncoder()\nfor i in cat_vars:\n    data[i]=data[[i]].apply(LE.fit_transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check to see if attributes have transformed\ndata[cat_vars].dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Scaling\nscaler=StandardScaler()\nscaler.fit(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaled=scaler.transform(data)\n#scaled_data is in an array format; We shall convert it back to a dataframe\nscaled_data=pd.DataFrame(scaled,columns=data.columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check is scaling was done correctly\nscaled_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Principal Component Analysis\n\nIt is difficult to visualize high dimensional data, so we can use PCA to find principal components.","metadata":{}},{"cell_type":"code","source":"#Reducing to 3 dimensions, so that we can plot a 3D graph\npca=PCA(n_components=3)\npca.fit(scaled_data)\npca_transformed=pca.transform(scaled_data)\npca_data=pd.DataFrame(pca_transformed,columns=['Att1','Att2','Att3'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig=plt.figure(figsize=(10,8))\nax=fig.add_subplot(projection='3d')\nax.scatter(pca_data['Att1'],pca_data['Att2'],pca_data['Att3'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Do we really need 3 principal components? We can check that using a scree plot to show percentage of total variance explained.","metadata":{}},{"cell_type":"code","source":"pc_values=np.arange(pca.n_components_)\nplt.plot(pc_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Variance Explained')\nplt.show()\n\nprint(pca.explained_variance_ratio_)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The first principal component explains around 40.3% of total variation in the dataset, the second explains 10.3% and the third explains 8.1%.","metadata":{}},{"cell_type":"markdown","source":"## Clustering\n\nWith the 3 attributes from dimensionality reduction, we can move on to perform clustering. I will use the elbow method to determine the optimal number of clusters.","metadata":{}},{"cell_type":"code","source":"km = KMeans()\nElbow_M = KElbowVisualizer(estimator = km, k = 10)\nElbow_M.fit(pca_data)\nElbow_M.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the graph, the optimal cluster size is 4, where there is a kink in the curve. After 4 clusters, the Within Cluster Sum of squares does not decrease significantly with each iteration.\n\n# Evaluate Model","metadata":{}},{"cell_type":"code","source":"#Assigning datapoints to each cluster\n\nkm=KMeans(n_clusters=4,random_state=0)\n\nprediction=km.fit_predict(pca_data)\npca_data['Cluster']=prediction\ndata['Cluster']=prediction","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize clusters in 3D plot\n\nfig=plt.figure(figsize=(10,8))\nax=fig.add_subplot(projection='3d')\nax.scatter(pca_data['Att1'],pca_data['Att2'],pca_data['Att3'],c=pca_data['Cluster'],marker='o',alpha=0.5,cmap='Accent')\nax.set_title('Clusters Plot 3D')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize clusters in 2D plot\n\nplt.figure(figsize=(10,8))\n\nsns.scatterplot(data=pca_data,x='Att1',y='Att2',hue='Cluster')\nplt.title('Clusters plot 2D')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It would be useful to identify features of the clusters for customer segmentation. This can be done through analyzing plots, to observe clustering of datapoints across attribute comparisons.","metadata":{}},{"cell_type":"code","source":"#Distribution of datapoints in clusters\n\ndist= sns.countplot(x=data[\"Cluster\"])\ndist.set_title('Distribution of clusters')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot1 = sns.scatterplot(data = data,x=data[\"Spending\"], y=data[\"Income\"],hue=data[\"Cluster\"], palette= 'Accent')","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Continuous variables\nexamine_vars=['Income','Spending','Age','Member_months']\nexamine_vars.append('Cluster')\np=sns.pairplot(data[examine_vars],hue='Cluster')\np.fig.set_size_inches(15,15)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Features=['Education', 'Marital_Status','Children','Is_parent']\nfor i in Features:\n    plt.figure()\n    sns.kdeplot(x=data[i],y=data['Spending'],hue=data['Cluster'],palette='Accent')\n    plt.title('{} vs Spending'.format(i))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nFrom our plots, it appears that the 4 clusters have the following characteristics:  \n\nCluster 0: Average Income, Average Spending, Mostly parents but maximum 2 children  \nCluster 1: High Income, High Spending, Mostly not parents and maximum of 1 child  \nCluster 2: Average to Low Income, Low Spending, Mostly parents with no restrictions on number of children  \nCluster 3: Low Income, Low Spending, a fair mix between parents and non-parents but no more than 2 chilren  ","metadata":{}}]}